{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(dataframe, duplicate_row_to_keep = \"first\", column_to_subset_by=\"time\"):\n",
    "   \n",
    "    # Resets the dataframe index, as a precaution. I don't like sudden jumps in index values\n",
    "    # converts time column to datetime object and sorts values in ascending order (chronologically)\n",
    "    # Removes the duplicates.\n",
    "    # 'duplicate_row_to_keep' may either be \"first\" or \"last\". The default is set to \"first\".\n",
    "    # 'column_to_subset_by' will be the column with the timestamp values. The default is set to \"time\".\n",
    "\n",
    "    \n",
    "    # Make the necessary imports\n",
    "    import pandas as pd\n",
    "    \n",
    "    #convert time column from str to datetime type\n",
    "    dataframe['time'] = pd.to_datetime(dataframe['time'])\n",
    "    dataframe = dataframe.sort_values(by=['time'])\n",
    "    \n",
    "    # remove duplicates\n",
    "    dataframe = dataframe.drop_duplicates(subset=column_to_subset_by, keep=duplicate_row_to_keep)\n",
    "    \n",
    "    # reset index\n",
    "    dataframe.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # return the duplicate removed dataframe\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Smoothing noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_noise(dataframe, window_size=5):\n",
    "    # smoothes noise by running a rolling mean and drops the null columns\n",
    "    # window_size is set to 5 as default\n",
    "    \n",
    "    # make necessary imports \n",
    "    import pandas as pd\n",
    "    \n",
    "    dataframe['x'] = animal1['x'].rolling(window=window_size).mean()\n",
    "    dataframe['y'] = animal1['y'].rolling(window=window_size).mean()\n",
    "    dataframe['z'] = animal1['z'].rolling(window=window_size).mean()\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating the x_diff, y_diff, z_diff and sum_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_differential_values(dataframe):\n",
    "    # renames x, y, z columns to add _diff with their names\n",
    "    # calculates their differential values\n",
    "    # calculates the sum of the (absolute) differential values\n",
    "    \n",
    "    # make necessary imports\n",
    "    import padas as pd\n",
    "    \n",
    "    # rename columns    \n",
    "    dataframe.rename(columns={\"x\":\"x_diff\", \"y\": \"y_diff\", \"z\": \"z_diff\"})\n",
    "    \n",
    "    # turn time to index\n",
    "    dataframe = dataframe.set_index('time')\n",
    "    \n",
    "    # now calculate the differences between consecutive rows\n",
    "    dataframe = dataframe.diff(axis=0, periods=1)\n",
    "    \n",
    "    # now drop rows with na values\n",
    "    dataframe = dataframe.dropna()\n",
    "    \n",
    "    # now calculate the sum_diff\n",
    "    dataframe['sum_diff'] = abs(dataframe['x_diff']) + abs(dataframe['y_diff']) + abs(dataframe['z_diff'])\n",
    "    \n",
    "    # return the changed dataframe\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clustering_function(km_df, number_of_clusters=3, init = \"random\", n_init=20):\n",
    "    # necessary imports\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn.cluster import KMeans\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    \n",
    "    # drop the time column \n",
    "    km_df = km_df.drop(columns = ['time']) \n",
    "    \n",
    "    # separate the data from the dataframe and convert to np array\n",
    "    X = km_df.values\n",
    "    X = np.nan_to_num(X)\n",
    "    \n",
    "    # scale the data\n",
    "    # try two different heuristics: the minmax scaler and the standard scaler \n",
    "    # and see which works better\n",
    "    # my hunch is the standard scaler should work better since the variables may have covariance\n",
    "    Clus_dataSet = StandardScaler().fit_transform(X)\n",
    "    Clus_dataSet\n",
    "    \n",
    "    # Initialize the number of clusters. My personal recommendation is 2.\n",
    "    clusterNum = number_of_clusters\n",
    "    \n",
    "    ## These are the parameters you can choose for initializing the Kmeans class\n",
    "    # init : {‘k-means++’, ‘random’ or an ndarray}\n",
    "    # Method for initialization, defaults to ‘k-means++’:\n",
    "    # ‘k-means++’ : selects initial cluster centers for k-mean clustering in a smart way to speed up convergence. \n",
    "    # See section Notes in k_init for more details.\n",
    "    # ‘random’: choose k observations (rows) at random from data for the initial centroids.\n",
    "    # If an ndarray is passed, it should be of shape (n_clusters, n_features) and gives the initial centers.\n",
    "    # n_init : int, default: 10\n",
    "    # Number of time the k-means algorithm will be run with different centroid seeds. \n",
    "    # The final results will be the best output of n_init consecutive runs in terms of inertia.\n",
    "    # max_iter : int, default: 300\n",
    "    \n",
    "    # initialize the kmeans model. \n",
    "    # for the time being, just tune the init, n_clusters and n_init parameters\n",
    "    # we'll find out more about the optimal number of clusters from the elbow method later\n",
    "    k_means = KMeans(init = init, n_clusters = clusterNum, n_init = n_init)\n",
    "    \n",
    "    # fit the model with the data\n",
    "    k_means.fit(X)\n",
    "    \n",
    "    # separate the labels\n",
    "    labels = k_means.labels_\n",
    "    \n",
    "    # add the labels into a new column to the original dataframe\n",
    "    # we need a labelled dataset to train the classifier model\n",
    "    km_df[\"Clus_km\"] = labels\n",
    "    \n",
    "    # return the labeled dataframe\n",
    "    return km_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_svc(svm_df):\n",
    "    # importing the libraries\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn.externals import joblib\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.svm import SVC\n",
    "    import os\n",
    "    \n",
    "    \n",
    "    # To divide the data into attributes and labels, execute the following code:\n",
    "    X = svm_df.drop(columns=['Clus_km'])\n",
    "    y = svm_df['Clus_km']\n",
    "    \n",
    "    # divide data into training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state = 101)\n",
    "    \n",
    "    # initialize the classifier\n",
    "    # use gridsearch on a test dataset to find the best parameters (see the script file)\n",
    "    svclassifier = SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
    "                      decision_function_shape='ovr', degree=3, gamma=1, kernel='rbf',\n",
    "                      max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
    "                      tol=0.001, verbose=False)\n",
    "    \n",
    "    # fit the classifier into the data\n",
    "    model = svclassifier.fit(X_train, y_train)\n",
    "    \n",
    "    # now predict on the test set using the predict method\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # save the svm model\n",
    "    path = os.getcwd()\n",
    "    joblib.dump(model, path + \"/svm_for_cow.pkl\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict using the SVC model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_using_svc(cow, classifier_filepath=\"svm_for_cow.pkl\"):\n",
    "    # Necessary imports\n",
    "    import datetime\n",
    "    import pandas as pd\n",
    "    from sklearn.externals import joblib\n",
    "    \n",
    "    # rename columns\n",
    "    cow.rename(columns={ \"x\": \"x_diff\" }, inplace = True)\n",
    "    cow.rename(columns={ \"y\": \"y_diff\" }, inplace = True)\n",
    "    cow.rename(columns={ \"z\": \"z_diff\" }, inplace = True)\n",
    "\n",
    "    # turn time to index\n",
    "    cow = cow.set_index('time')\n",
    "\n",
    "    # now calculate the differences between consecutive rows\n",
    "    cow = cow.diff(axis=0, periods=1)\n",
    "\n",
    "    # now calculate the sum_diff\n",
    "    cow['sum_diff'] = abs(cow['x_diff']) + abs(cow['y_diff']) + abs(cow['z_diff'])\n",
    "\n",
    "    # now drop rows with na values\n",
    "    cow = cow.dropna()\n",
    "    \n",
    "    # Load model from file\n",
    "    classifier = joblib.load(classifier_filepath)\n",
    "    \n",
    "    # now predict on the test set using the predict method\n",
    "    y_pred = classifier.predict(cow)\n",
    "    \n",
    "    # add labels to the dataframe\n",
    "    labels = y_pred\n",
    "    cow['labels'] = labels\n",
    "    \n",
    "    # returned the labeled dataframe\n",
    "    return cow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tabulate weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_weights(cow):\n",
    "    # imports\n",
    "    import pandas as pd\n",
    "    from datetime import date\n",
    "    from datetime import time\n",
    "    from datetime import datetime\n",
    "    from datetime import timedelta\n",
    "    \n",
    "    # initialize the weight column\n",
    "    cow['weight'] = None\n",
    "    \n",
    "    # tabulate the values in the weight column\n",
    "    # assumption is cluster 0 is low activity, cluster 1 is medium activity, cluster 2 is high activity\n",
    "    for i in range(len(cow)):\n",
    "        if (cow.loc[i, 'time']==0):\n",
    "            cow.loc[i, 'weight']= 0\n",
    "        elif (cow.loc[i, 'time']==1):\n",
    "            cow.loc[i, 'weight']= 0.1\n",
    "        else:\n",
    "            cow.loc[i, 'weight']= 0.9\n",
    "            \n",
    "    # return the weight added dataframe\n",
    "    return cow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating Activity Level in 1 hour time slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_value_counts(cow):\n",
    "    # imports \n",
    "    import pandas as pd\n",
    "    from datetime import date\n",
    "    from datetime import time\n",
    "    from datetime import datetime\n",
    "    from datetime import timedelta\n",
    "    \n",
    "    # convert time column from str to datetime type\n",
    "    # sort in ascending order (chronologically)\n",
    "    cow['time'] = pd.to_datetime(cow['time'])\n",
    "    cow = cow.sort_values(by=['time'])\n",
    "    \n",
    "    # find the initial time value\n",
    "    starting_time = cow.loc[0, 'time']\n",
    "    \n",
    "    # initialize the lists that are to be appended in the loop\n",
    "    time = []\n",
    "    activity_level = []\n",
    "    \n",
    "    # create one hour time slices\n",
    "    end_time = starting_time\n",
    "    while (end_time <= cow.iloc[len(cow)-1, 0]):\n",
    "        # create the 1-hour time range for the data to be filtered in\n",
    "        new_time = end_time\n",
    "        end_time = new_time + timedelta(hours=1) # one hour slice\n",
    "        \n",
    "        # create date filter mask\n",
    "        # greater than the start date and smaller than the end date\n",
    "        # hold the data from the 1-hour timeslice in a placeholder dataframe\n",
    "        mask = (cow['time'] > new_time) & (cow['time'] <= end_time)\n",
    "        placeholder = cow[mask]\n",
    "        \n",
    "        # summarise the value counts for the labels in the placeholder dataframe\n",
    "        summary = pd.DataFrame(placeholder['labels'].value_counts())\n",
    "        summary['cluster'] = summary.index\n",
    "        \n",
    "        # rename columns\n",
    "        summary.rename(columns={ summary.columns[0]: \"count\" }, inplace = True)\n",
    "\n",
    "        # add weights column\n",
    "        # tabulate the values in the weight column\n",
    "        # IMPORTANT: assumption is cluster 0 is low activity, cluster 1 is medium activity, cluster 2 is high activity\n",
    "        summary['weight'] = None\n",
    "        for i in range(len(summary)):\n",
    "            if (summary.iloc[i, 1]==0):\n",
    "                summary.loc[i, 'weight']=0\n",
    "            elif (summary.iloc[i, 1]==1):\n",
    "                summary.loc[i, 'weight']=0.1\n",
    "            else:\n",
    "                summary.loc[i, 'weight']=0.9\n",
    "\n",
    "        \n",
    "        # initialise the hourly activity level as an empty list\n",
    "        hourly_activity_level = []\n",
    "\n",
    "        \n",
    "        # keep appending the product of weight and value count for each cluster label to the hourly_activity_level list\n",
    "        for i in range(len(summary)):\n",
    "            activity_level.append(summary.iloc[i, 0]*summary.iloc[i, 2])\n",
    "\n",
    "        # calculate the sum of the elements in the hourly_activity_level list\n",
    "        hourly_activity_level = sum(hourly_activity_level)\n",
    "    \n",
    "        # append the time and sum of hourly activity level to the time and activity_level lists\n",
    "        time.append(new_time)\n",
    "        activity_level.append(hourly_activity_level)\n",
    "\n",
    "        # create a dataframe using the time and activity level lists\n",
    "        activity_df = pd.DataFrame(list(zip(time, activity_level)), \n",
    "                                   columns =['time', 'activity_level'])\n",
    "        \n",
    "        # convert the time column to datetime object\n",
    "        activity_df['time'] = pd.to_datetime(activity_df['time'])\n",
    "        \n",
    "        # initialise the activity level columns for the previous 1, 24, 48, and 72 hours\n",
    "        activity_df['activity_level_1'] = None\n",
    "        activity_df['activity_level_24'] = None\n",
    "        activity_df['activity_level_48'] = None\n",
    "        activity_df['activity_level_72'] = None\n",
    "        \n",
    "        for i in range(len(activity_df)):\n",
    "\n",
    "            timevalue_1 = activity_df.loc[i, 'time'] - timedelta(hours=1)\n",
    "            timevalue_24 = activity_df.loc[i, 'time'] - timedelta(hours=24)\n",
    "            timevalue_48 = activity_df.loc[i, 'time'] - timedelta(hours=48)\n",
    "            timevalue_72 = activity_df.loc[i, 'time'] - timedelta(hours=72)\n",
    "\n",
    "            # some errors arise due to duplicate time values being present in the data\n",
    "            # trying to cheat my way through with the use of try except block\n",
    "            try:  \n",
    "                activity_df.loc[i, 'activity_level_1'] = activity_df.loc[activity_df['time']==timevalue_1]['activity_level'].values[0]\n",
    "                activity_df.loc[i, 'activity_level_24'] = activity_df.loc[activity_df['time']==timevalue_24]['activity_level'].values[0]\n",
    "                activity_df.loc[i, 'activity_level_48'] = activity_df.loc[activity_df['time']==timevalue_48]['activity_level'].values[0]\n",
    "                activity_df.loc[i, 'activity_level_72'] = activity_df.loc[activity_df['time']==timevalue_72]['activity_level'].values[0]\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "    \n",
    "    # return the activity level dataframe\n",
    "    return activity_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate the Activity Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_activity_index(activity_df):\n",
    "    # imports\n",
    "    import pandas as pd\n",
    "    from datetime import date\n",
    "    from datetime import time\n",
    "    from datetime import datetime\n",
    "    from datetime import timedelta\n",
    "    \n",
    "    # initialize the historical comparison value, trend and activity index columns\n",
    "    activity_df['historical_comparison_value'] = None\n",
    "    activity_df['trend'] = None\n",
    "    activity_df['activity_index'] = None\n",
    "    \n",
    "    # calculate the historical comparison value, trend and activity index columns\n",
    "    for i in range(len(activity_df)):\n",
    "\n",
    "        # calculate the historical comparison value\n",
    "        try:\n",
    "            placeholder = 3*activity_df.loc[i, 'activity_level']\n",
    "            placeholder = placeholder - (activity_df.loc[i, 'activity_level_24'] + activity_df.loc[i, 'activity_level_48'] + activity_df.loc[i, 'activity_level_72'])\n",
    "            historical_comparison_value = placeholder/(activity_df.loc[i, 'activity_level_24'] + activity_df.loc[i, 'activity_level_48'] + activity_df.loc[i, 'activity_level_72'])\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # calculate the trend\n",
    "        try:\n",
    "            trend = (activity_df.loc[i, 'activity_level'] - activity_df.loc[i, 'activity_level_1'])/activity_df.loc[i, 'activity_level_1']\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # insert the values for historical comparison value and trend into the dataframe\n",
    "        try:\n",
    "            activity_df.loc[i, 'historical_comparison_value'] = historical_comparison_value\n",
    "            activity_df.loc[i, 'trend'] = trend\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "\n",
    "        # calculate the activity index now\n",
    "        try:\n",
    "            activity_df.loc[i, 'activity_index'] = historical_comparison_value + trend\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # create an activity index column\n",
    "    # drop unnecessary columns\n",
    "    activity_index_df = activity_df.drop(columns=['activity_level', 'activity_level_1',\n",
    "                                             'activity_level_24', 'activity_level_48',\n",
    "                                             'activity_level_72', 'historical_comparison_value',\n",
    "                                             'trend'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
